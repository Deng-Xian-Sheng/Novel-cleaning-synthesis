{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682e1288-c587-4aed-adb9-d22a53306daa",
   "metadata": {},
   "source": [
    "该ipynb未完成，目前的问题：\n",
    "\n",
    "https://chatgpt.com/s/t_6992ef18f3c08191a88814dd6f668b9a\n",
    "https://chatgpt.com/s/t_6992ef369e948191bba754b0ce532ba6\n",
    "https://chatgpt.com/s/t_6992ef4e91988191a3e98c8944d6434f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b12090-5bc9-4899-84da-46187597c631",
   "metadata": {},
   "source": [
    "# GRPO：仅用 Embedding 相似度奖励（你的 Qwen3-4B-Thinking 训练后整模 + Qwen3-VL-Embedding-2B）\n",
    "\n",
    "变化点（相对你之前看的 Unslo†h notebook）：\n",
    "1) 数据集：读取 jsonl，每行一个 `messages`（OpenAI messages），单轮对话\n",
    "2) prompt：直接用 `messages[:-1]`（不额外加 system）\n",
    "3) reward：对 completion 做 “剥离思维链后的最终输出” -> embedding -> 与参考答案 embedding 相似度\n",
    "4) 工程：同一张卡上跑两个 vLLM（生成 + embedding），必须拆分 `gpu_memory_utilization`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5c90c-9052-49c4-925f-b5b67b44bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2819b-b8da-4270-82ed-e0c8e13e7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 1) 安装依赖\n",
    "# =======================\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install -U \"unsloth\" \"unsloth-zoo\" \\\n",
    "  \"vllm==0.11.2\" \\\n",
    "  \"trl==0.22.2\" \\\n",
    "  \"transformers==4.57.1\" \\\n",
    "  \"qwen-vl-utils>=0.0.14\" \"datasets\" \"accelerate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429b7a8-62bb-4c99-8ebe-4b31120fe98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52cbcc-ba17-4b41-b501-161b7c483325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('unsloth/Qwen3-Embedding-0.6B', local_dir=\"./Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9c0e7-32c4-4bca-b655-1677fcae0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"  # 禁用 vLLM V1，引导走旧引擎，新引擎与trl不兼容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab58b64-70c1-4190-a1e8-25476f7a6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 只从本地读取模型文件，防止hf连不上\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b56be-2d07-43d1-9ee8-a6c449208d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 防止pytorch保留太多显存\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5afd5-85ae-4d8f-a9fc-94d5f47aa009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2) 基本配置（按你本地改路径）\n",
    "# =======================\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 你的整模（已 SFT 全量微调过，并保存成 vLLM/HF 能加载的目录）\n",
    "GEN_MODEL_PATH = \"/root/autodl-tmp/lora_model\" # lora_model文件夹存的是完整权重，只是名字是lora_model\n",
    "\n",
    "# 你的 jsonl：每行一个对象，至少包含 {\"messages\": [...]}（单轮对话）\n",
    "DATA_JSONL_PATH = \"/root/autodl-tmp/dataset.segmented.jsonl\"\n",
    "\n",
    "# embedding 模型\n",
    "EMB_MODEL_NAME = \"./Qwen3-Embedding-0.6B\"\n",
    "\n",
    "# ===== 显存关键：两个 vLLM 实例要拆分占用 =====\n",
    "# 32GB 参考：生成 0.65~0.75，embedding 0.20~0.30 之间调\n",
    "GEN_GPU_MEM_UTIL = 0.65\n",
    "EMB_GPU_MEM_UTIL = 0.20\n",
    "\n",
    "# ===== 训练相关 =====\n",
    "max_seq_length = 8192\n",
    "lora_rank = 128\n",
    "\n",
    "num_generations = 4\n",
    "max_steps = 12000\n",
    "save_steps = 1200\n",
    "\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "\n",
    "# KL 约束：建议很小，防止跑偏\n",
    "beta_kl = 0.05\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08466e0-82ee-4b0d-a266-c17ea7eaca76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 3) 加载 embedding 模型（Qwen3-Embedding-0.6B / vLLM task=embed）\n",
    "# =======================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vllm import LLM\n",
    "\n",
    "class VLLMTextEmbedder:\n",
    "    \"\"\"\n",
    "    Qwen3-Embedding-* 推荐用纯文本输入：\n",
    "      \"Instruct: ...\\nQuery: ...\"\n",
    "    vLLM 侧直接 task=\"embed\"，不需要 runner=\"pooling\" / apply_chat_template。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        dtype: str = \"bfloat16\",\n",
    "        gpu_memory_utilization: float = 0.25,\n",
    "        instruction: str = \"Represent the text for semantic similarity.\",\n",
    "        max_model_len: int = 8192,   #  Qwen3-Embedding-0.6B 支持 32k，但不需要那么多，所以设小点，省显存\n",
    "    ):\n",
    "        self.instruction = instruction\n",
    "\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            task=\"embed\",\n",
    "            dtype=dtype,\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            max_model_len=max_model_len,\n",
    "        )\n",
    "\n",
    "    def _format(self, text: str) -> str:\n",
    "        # ⚠️ 为了让“参考答案 embedding”和“模型输出 embedding”在同一空间，\n",
    "        # 建议两边都用同一个格式（要么都带 instruction，要么都不带）。\n",
    "        if self.instruction and self.instruction.strip():\n",
    "            return f\"Instruct: {self.instruction}\\nQuery: {text}\"\n",
    "        return text\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts, normalize=True):\n",
    "        inputs = [self._format(t) for t in texts]\n",
    "        outs = self.llm.embed(inputs)\n",
    "        embs = torch.tensor([o.outputs.embedding for o in outs],\n",
    "                            dtype=torch.float32,\n",
    "                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if normalize:\n",
    "            embs = F.normalize(embs, p=2, dim=-1)\n",
    "        return embs\n",
    "\n",
    "\n",
    "# 建议按官方推荐：instruction 用英文（训练时大多是英文 instruction）\n",
    "EMB_INSTRUCTION = \"Represent the text for semantic similarity.\"\n",
    "\n",
    "embedder = VLLMTextEmbedder(\n",
    "    EMB_MODEL_NAME,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=EMB_GPU_MEM_UTIL,\n",
    "    instruction=EMB_INSTRUCTION,\n",
    "    max_model_len=max_seq_length, # 比grpo时模型的输出(max_seq_length - max_prompt_length)长一点，防止模型输出超过Embedder的上下文上限\n",
    ")\n",
    "\n",
    "print(\"Loaded embedding model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bccf7-0739-4484-af3d-4d7c3e0b60bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4) 加载生成模型（Unsloth + LoRA）\n",
    "# =======================\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = GEN_MODEL_PATH,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,                 # vLLM rollout\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = GEN_GPU_MEM_UTIL,  # 关键：给生成模型的 vLLM 限额\n",
    "    # local_files_only = True,\n",
    ")\n",
    "\n",
    "# TRL 建议 left padding（见 TRL dataset/grpo 说明）\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"Loaded gen model + LoRA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a27f91-2e48-4e92-9c3c-b1f7dc0bef8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5) 读取 jsonl messages 数据集，并转成 GRPOTrainer 格式\n",
    "#    修复点：map 时 remove_columns，把原始 \"messages\"（以及其它原始列）删掉\n",
    "# =======================\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw0 = load_dataset(\"json\", data_files=DATA_JSONL_PATH, split=\"train\")\n",
    "orig_cols = raw0.column_names  # 通常包含 [\"messages\"]，也可能还有别的列\n",
    "\n",
    "def to_grpo_format(example, idx):\n",
    "    msgs = example.get(\"messages\", None)\n",
    "\n",
    "    # 防御：坏样本直接变空，后面 filter 会丢掉\n",
    "    if not isinstance(msgs, list) or len(msgs) < 2:\n",
    "        return {\"row_id\": idx, \"prompt\": [], \"answer_text\": \"\"}\n",
    "\n",
    "    # prompt：去掉最后一条 assistant（参考答案）\n",
    "    prompt_msgs = msgs[:-1]\n",
    "\n",
    "    # 可选：如果你坚决不要 system，就过滤掉\n",
    "    prompt_msgs = [m for m in prompt_msgs if m.get(\"role\") != \"system\"]\n",
    "\n",
    "    # 参考答案：最后一条 assistant\n",
    "    answer_msg = msgs[-1]\n",
    "    if isinstance(answer_msg, dict):\n",
    "        answer_text = str(answer_msg.get(\"content\", \"\"))\n",
    "    else:\n",
    "        answer_text = str(answer_msg)\n",
    "\n",
    "    return {\n",
    "        \"row_id\": idx,\n",
    "        \"prompt\": prompt_msgs,      # TRL 支持：prompt 直接用 messages 列表\n",
    "        \"answer_text\": answer_text,\n",
    "    }\n",
    "\n",
    "raw = raw0.map(\n",
    "    to_grpo_format,\n",
    "    with_indices=True,\n",
    "    remove_columns=orig_cols,      # 关键：删掉原始列，避免同时存在 messages + prompt\n",
    "    desc=\"Convert to GRPO format\",\n",
    ")\n",
    "\n",
    "# 过滤空样本\n",
    "raw = raw.filter(lambda x: len(x[\"prompt\"]) > 0 and len(x[\"answer_text\"].strip()) > 0)\n",
    "\n",
    "print(\"keys:\", raw.column_names)  # 期望：['row_id','prompt','answer_text']\n",
    "print(raw[0])\n",
    "print(\"N =\", len(raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eccdf5-2488-4c4a-a2fe-d441d81050a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc4e0f-9e9a-47cb-9316-74374c3ee945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 7) 剥离思维链 + reward（embedding 相似度）\n",
    "# =======================\n",
    "\n",
    "THINK_END_TAGS = [\n",
    "    \"</think>\",\n",
    "    \"<end_working_out>\",\n",
    "]\n",
    "\n",
    "def strip_thought(text: str, tail_chars: int = 3000) -> str:\n",
    "    t = text\n",
    "\n",
    "    # 命中 </think> / <end_working_out> 时：\n",
    "    # - 若 tag 之后确实有内容：返回“全部内容”（不再截断 3000）\n",
    "    # - 若 split 后为空（或全是空白）：保持原逻辑（走尾部 tail_chars 回退）\n",
    "    for tag in THINK_END_TAGS:\n",
    "        if tag in t:\n",
    "            after = t.split(tag, 1)[1]\n",
    "            after_stripped = after.strip()\n",
    "            if after_stripped:\n",
    "                return after_stripped\n",
    "            t = after  # 为空则继续走原逻辑回退\n",
    "            break\n",
    "\n",
    "    # 无 tag：保持原始逻辑不变（只取尾部 tail_chars）\n",
    "    t = t.strip()\n",
    "    t = t[-tail_chars:].strip() if t else text[-tail_chars:].strip()\n",
    "    return t\n",
    "\n",
    "# -----------------------\n",
    "# tokenizer 版 repetition penalty\n",
    "# -----------------------\n",
    "def repetition_penalty_from_ids(token_ids, min_tokens: int = 20) -> float:\n",
    "    \"\"\"\n",
    "    用 token_id bigram 多样性检测复读：\n",
    "    ratio = uniq_bigrams / total_bigrams\n",
    "    \"\"\"\n",
    "    if not token_ids:\n",
    "        return -2.0\n",
    "\n",
    "    n = len(token_ids)\n",
    "    if n < min_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    total = n - 1\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    uniq = len({(token_ids[i], token_ids[i + 1]) for i in range(total)})\n",
    "    ratio = uniq / max(1, total)\n",
    "\n",
    "    if ratio < 0.3:\n",
    "        return -2.0\n",
    "    if ratio < 0.5:\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "\n",
    "# -----------------------\n",
    "# completions_ids 版 repetition penalty（无需 tokenizer.encode）\n",
    "# - 尽量模拟你当前 strip_thought 的“tag 后内容”逻辑\n",
    "# - 若找不到 tag，则退化为取尾部 tail_tokens\n",
    "# -----------------------\n",
    "def _find_subseq_end(seq, pattern):\n",
    "    \"\"\"返回 pattern 在 seq 中首次出现的位置的“结束下标”(i+len(pattern))；找不到返回 -1。\"\"\"\n",
    "    m = len(pattern)\n",
    "    if m == 0:\n",
    "        return -1\n",
    "    # 朴素搜索，completion 一般不长，足够用\n",
    "    for i in range(0, len(seq) - m + 1):\n",
    "        if seq[i : i + m] == pattern:\n",
    "            return i + m\n",
    "    return -1\n",
    "\n",
    "def repetition_penalty_completions_ids_batch(\n",
    "    completions_ids,\n",
    "    tokenizer,\n",
    "    think_end_tag_id_seqs,\n",
    "    empty_mask,\n",
    "    min_tokens: int = 20,\n",
    "    tail_tokens: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "      completions_ids: list[list[int]] 或 torch.Tensor(B, L)\n",
    "    输出：\n",
    "      penalties: list[float]\n",
    "    \"\"\"\n",
    "    # 兼容 tensor\n",
    "    if isinstance(completions_ids, torch.Tensor):\n",
    "        ids_list = completions_ids.detach().cpu().tolist()\n",
    "    else:\n",
    "        ids_list = completions_ids\n",
    "\n",
    "    pad_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "    eos_id = getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    penalties = []\n",
    "    for ids, is_empty in zip(ids_list, empty_mask):\n",
    "        if is_empty:\n",
    "            penalties.append(-2.0)\n",
    "            continue\n",
    "\n",
    "        # 清掉 padding / -100（有些实现会用 -100 做 ignore）\n",
    "        cleaned = []\n",
    "        for t in ids:\n",
    "            if t == -100:\n",
    "                continue\n",
    "            if pad_id is not None and t == pad_id:\n",
    "                continue\n",
    "            cleaned.append(t)\n",
    "\n",
    "        # 去掉末尾 eos（避免 eos 重复影响 bigram）\n",
    "        if eos_id is not None:\n",
    "            while cleaned and cleaned[-1] == eos_id:\n",
    "                cleaned.pop()\n",
    "\n",
    "        # 找 </think> / <end_working_out> 等 tag 的 token 序列，截取 tag 后内容\n",
    "        cut_pos = -1\n",
    "        for tag_ids in think_end_tag_id_seqs:\n",
    "            p = _find_subseq_end(cleaned, tag_ids)\n",
    "            if p != -1:\n",
    "                cut_pos = p\n",
    "                break  # 命中一个就用第一个命中的\n",
    "\n",
    "        if cut_pos != -1:\n",
    "            eff = cleaned[cut_pos:]\n",
    "        else:\n",
    "            eff = cleaned[-tail_tokens:] if tail_tokens > 0 else cleaned\n",
    "\n",
    "        penalties.append(repetition_penalty_from_ids(eff, min_tokens=min_tokens))\n",
    "\n",
    "    return penalties\n",
    "\n",
    "def repetition_penalty_tokenizer_batch(\n",
    "    texts,\n",
    "    tokenizer,\n",
    "    min_tokens: int = 20,\n",
    "    tail_tokens: int = 512,   # 只看尾部 N tokens：更贴近“复读常发生在结尾”，也更省时\n",
    "):\n",
    "    \"\"\"\n",
    "    批量用 tokenizer 编码，然后对每条算 repetition penalty。\n",
    "    - add_special_tokens=False：避免特殊 token 干扰统计\n",
    "    - truncation_side='left'：保留尾部 token（更有价值）\n",
    "    \"\"\"\n",
    "    # 空文本沿用你旧逻辑：直接 -2.0\n",
    "    empty_mask = [(t is None) or (len(str(t).strip()) == 0) for t in texts]\n",
    "    safe_texts = [(\"\" if e else str(t)) for t, e in zip(texts, empty_mask)]\n",
    "\n",
    "    # 临时把 truncation_side 设为 left，确保 max_length 截断时保留尾部 token\n",
    "    old_trunc_side = getattr(tokenizer, \"truncation_side\", \"right\")\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    enc = tokenizer(\n",
    "        safe_texts,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=tail_tokens,\n",
    "        padding=False,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    tokenizer.truncation_side = old_trunc_side\n",
    "\n",
    "    ids_list = enc[\"input_ids\"]\n",
    "\n",
    "    penalties = []\n",
    "    for ids, is_empty in zip(ids_list, empty_mask):\n",
    "        if is_empty:\n",
    "            penalties.append(-2.0)\n",
    "        else:\n",
    "            penalties.append(repetition_penalty_from_ids(ids, min_tokens=min_tokens))\n",
    "    return penalties\n",
    "\n",
    "@torch.no_grad()\n",
    "def reward_embedding_similarity(prompts, completions, row_id, answer_text, completions_ids=None, **kwargs):\n",
    "    # completions: list[ list[{role, content}] ]（通常是 assistant message）\n",
    "    comp_texts = []\n",
    "    for c in completions:\n",
    "        if isinstance(c, str):\n",
    "            comp_texts.append(c)\n",
    "        else:\n",
    "            comp_texts.append(c[0][\"content\"])\n",
    "\n",
    "    final_texts = [strip_thought(t) for t in comp_texts]\n",
    "    empty_mask = [1 if len(t.strip()) == 0 else 0 for t in final_texts]\n",
    "\n",
    "    # 1) 模型输出 embedding（每条 completion 都要算）\n",
    "    emb_out = embedder.embed_texts(final_texts, normalize=True)\n",
    "\n",
    "    # 2) 参考答案 embedding：按 batch 去重计算，然后复用到 num_generations\n",
    "    #    row_id 会在 GRPO 中重复（每条 prompt 对应 num_generations 个 completion）\n",
    "    rid2pos = {}\n",
    "    uniq_answers = []\n",
    "    for rid, ans in zip(row_id, answer_text):\n",
    "        if rid not in rid2pos:\n",
    "            rid2pos[rid] = len(uniq_answers)\n",
    "            uniq_answers.append(ans)\n",
    "\n",
    "    emb_tgt_uniq = embedder.embed_texts(uniq_answers, normalize=True)\n",
    "\n",
    "    idx = torch.tensor(\n",
    "        [rid2pos[rid] for rid in row_id],\n",
    "        device=emb_tgt_uniq.device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    emb_tgt = emb_tgt_uniq.index_select(0, idx)\n",
    "\n",
    "    # cosine（两边都 normalize 过）\n",
    "    sim = (emb_out * emb_tgt).sum(dim=-1)\n",
    "    reward = 10.0 * sim\n",
    "\n",
    "    # 3) repetition penalty：优先用 TRL 传入的 completions_ids（无需 tokenizer）\n",
    "    if completions_ids is None:\n",
    "        completions_ids = kwargs.get(\"completions_ids\", None)\n",
    "\n",
    "    if completions_ids is not None:\n",
    "        # 把 think end tag 编成 token 序列\n",
    "        think_end_tag_id_seqs = [tokenizer.encode(tag, add_special_tokens=False) for tag in THINK_END_TAGS]\n",
    "\n",
    "        pen = repetition_penalty_completions_ids_batch(\n",
    "            completions_ids=completions_ids,\n",
    "            tokenizer=tokenizer,\n",
    "            think_end_tag_id_seqs=think_end_tag_id_seqs,\n",
    "            empty_mask=empty_mask,\n",
    "            min_tokens=20,\n",
    "            tail_tokens=512,\n",
    "        )\n",
    "    else:\n",
    "        # fallback：如果没传 completions_ids，就退回 tokenizer 版\n",
    "        pen = repetition_penalty_tokenizer_batch(\n",
    "            final_texts,\n",
    "            tokenizer,\n",
    "            min_tokens=20,\n",
    "            tail_tokens=512,\n",
    "        )\n",
    "\n",
    "    reward = reward + torch.tensor(pen, device=reward.device, dtype=reward.dtype)\n",
    "\n",
    "    # 空输出额外惩罚（保留你原逻辑）\n",
    "    reward = reward + torch.tensor(\n",
    "        [-3.0 if e else 0.0 for e in empty_mask],\n",
    "        device=reward.device,\n",
    "        dtype=reward.dtype,\n",
    "    )\n",
    "\n",
    "    return reward.detach().cpu().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e787470-bfaa-4b05-b85e-bd6ece063c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 8) 自检 reward 是否合理\n",
    "# =======================\n",
    "sample = raw[0]\n",
    "print(\"PROMPT:\", sample[\"prompt\"])\n",
    "print(\"\\nANSWER:\\n\", sample[\"answer_text\"])\n",
    "\n",
    "fake_completions = [\n",
    "    [{\"role\":\"assistant\",\"content\": f\"</think>\\n{sample['answer_text']}\"}],\n",
    "    [{\"role\":\"assistant\",\"content\": \"</think>\\n我不太确定，但大致是另一个意思。\"}],\n",
    "]\n",
    "\n",
    "r = reward_embedding_similarity(\n",
    "    prompts=[sample[\"prompt\"], sample[\"prompt\"]],\n",
    "    completions=fake_completions,\n",
    "    row_id=[sample[\"row_id\"], sample[\"row_id\"]],\n",
    "    answer_text=[sample[\"answer_text\"], sample[\"answer_text\"]],\n",
    ")\n",
    "print(\"\\nRewards:\", r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808dbc8-6d4c-42b1-a49c-cc2933d73488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 在grpo正在训练的情况下，测试训练的效果\n",
    "# =======================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class OnDemandGenerateCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    单卡训练中按需推理（不会常驻占显存，不会频繁生成）：\n",
    "    - 你往 request_path 追加一行 JSON（jsonl）= 投递一个生成请求\n",
    "    - 训练在 step_end 的“安全时机”轮询到请求后，用当前训练权重 generate\n",
    "    - 把结果追加写入 result_path（jsonl），并可选打印到 stdout\n",
    "    - 支持 start_from_end：重启训练后默认从 requests 文件末尾开始消费，避免回放旧请求\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        request_path=\"grpo_eval_requests.jsonl\",\n",
    "        result_path=\"grpo_eval_results.jsonl\",\n",
    "        poll_interval_sec=5.0,     # 多久检查一次 inbox（越小越“随时”，但开销略增）\n",
    "        max_requests_per_poll=1,   # 单卡建议 1，避免一次生成太久拖训练\n",
    "        default_gen_kwargs=None,\n",
    "        enable_thinking=False,     # Qwen3-4B-Thinking-2507的模型卡说过它会始终输出思维链，所以不需要设置True\n",
    "        start_from_end=True,       # 重启从文件末尾开始读\n",
    "        print_outputs=True,        # 是否在训练日志里打印输出\n",
    "        keep_full_text=True,      # True: 保存 full_text（含prompt）; False: 仅保存生成部分\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.request_path = request_path\n",
    "        self.result_path = result_path\n",
    "        self.poll_interval_sec = poll_interval_sec\n",
    "        self.max_requests_per_poll = max_requests_per_poll\n",
    "        self.default_gen_kwargs = default_gen_kwargs or dict(\n",
    "            max_new_tokens=192,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            top_k=20,\n",
    "        )\n",
    "        self.enable_thinking = enable_thinking\n",
    "        self.print_outputs = print_outputs\n",
    "        self.keep_full_text = keep_full_text\n",
    "\n",
    "        self._last_poll_t = 0.0\n",
    "        self._offset = 0\n",
    "\n",
    "        # 重启后不回放历史请求：从文件末尾开始读\n",
    "        if start_from_end and os.path.exists(self.request_path):\n",
    "            self._offset = os.path.getsize(self.request_path)\n",
    "\n",
    "    def _read_new_requests(self):\n",
    "        if not os.path.exists(self.request_path):\n",
    "            return []\n",
    "\n",
    "        reqs = []\n",
    "        with open(self.request_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.seek(self._offset)\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    reqs.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    reqs.append({\"_bad_line\": line[:300]})\n",
    "                if len(reqs) >= self.max_requests_per_poll:\n",
    "                    break\n",
    "            self._offset = f.tell()\n",
    "        return reqs\n",
    "\n",
    "    def _append_result(self, obj):\n",
    "        os.makedirs(os.path.dirname(self.result_path) or \".\", exist_ok=True)\n",
    "        with open(self.result_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        now = time.time()\n",
    "        if now - self._last_poll_t < self.poll_interval_sec:\n",
    "            return control\n",
    "        self._last_poll_t = now\n",
    "\n",
    "        reqs = self._read_new_requests()\n",
    "        if not reqs:\n",
    "            return control\n",
    "\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        if model is None:\n",
    "            return control\n",
    "\n",
    "        # 拿 device（比 model.device 更通用）\n",
    "        try:\n",
    "            device = next(model.parameters()).device\n",
    "        except StopIteration:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        for req in reqs:\n",
    "            if \"_bad_line\" in req:\n",
    "                self._append_result({\n",
    "                    \"ok\": False,\n",
    "                    \"step\": int(state.global_step),\n",
    "                    \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"error\": f\"bad_json_line: {req['_bad_line']}\",\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            req_id = req.get(\"id\") or f\"manual_{uuid.uuid4().hex[:8]}\"\n",
    "            gen_kwargs = dict(self.default_gen_kwargs)\n",
    "            gen_kwargs.update(req.get(\"gen\", {}))\n",
    "\n",
    "            # 允许两种请求格式：\n",
    "            # 1) {\"prompt\": \"...\"}\n",
    "            # 2) {\"messages\": [...]}\n",
    "            if \"messages\" in req:\n",
    "                messages = req[\"messages\"]\n",
    "            else:\n",
    "                messages = [{\"role\": \"user\", \"content\": req.get(\"prompt\", \"\")}]\n",
    "\n",
    "            try:\n",
    "                inputs = None\n",
    "                out_ids = None\n",
    "                gen_ids = None\n",
    "                \n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    enable_thinking=self.enable_thinking,\n",
    "                )\n",
    "\n",
    "                inputs = self.tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)\n",
    "\n",
    "                prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    out_ids = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "                # 只取生成部分（等价于 TextStreamer(skip_prompt=True) 的效果）\n",
    "                gen_ids = out_ids[0, prompt_len:]\n",
    "                gen_text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "                # 可选：也保存“剥离思维链后的版本”\n",
    "                gen_text_stripped = strip_thought(gen_text)\n",
    "\n",
    "                result = {\n",
    "                    \"ok\": True,\n",
    "                    \"id\": req_id,\n",
    "                    \"step\": int(state.global_step),\n",
    "                    \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"request\": req,\n",
    "                    \"gen_text\": gen_text,\n",
    "                    \"gen_text_stripped\": gen_text_stripped,\n",
    "                }\n",
    "\n",
    "                if self.keep_full_text:\n",
    "                    full_text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "                    result[\"full_text\"] = full_text\n",
    "\n",
    "                self._append_result(result)\n",
    "\n",
    "                if self.print_outputs:\n",
    "                    print(f\"\\n\\n===== [on-demand gen | step {state.global_step} | id={req_id}] =====\")\n",
    "                    print(gen_text_stripped)\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                self._append_result({\n",
    "                    \"ok\": False,\n",
    "                    \"id\": req_id,\n",
    "                    \"step\": int(state.global_step),\n",
    "                    \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"error\": \"CUDA OOM: try smaller max_new_tokens / temperature / shorter prompt\",\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                self._append_result({\n",
    "                    \"ok\": False,\n",
    "                    \"id\": req_id,\n",
    "                    \"step\": int(state.global_step),\n",
    "                    \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"error\": repr(e),\n",
    "                })\n",
    "\n",
    "            finally:\n",
    "                del out_ids, inputs, gen_ids\n",
    "                gc.collect()                \n",
    "\n",
    "        # 缓解碎片\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2bca7-81a3-49ec-af4d-6649ffcb6ef4",
   "metadata": {},
   "source": [
    "### 触发“在grpo正在训练的情况下，测试训练的效果”\n",
    "\n",
    "```python\n",
    "import json, time, uuid\n",
    "\n",
    "REQUEST_PATH = \"grpo_eval_requests.jsonl\"\n",
    "\n",
    "def enqueue_prompt(prompt: str, gen=None, req_id=None, request_path=REQUEST_PATH):\n",
    "    req = {\n",
    "        \"id\": req_id or f\"manual_{int(time.time())}_{uuid.uuid4().hex[:6]}\",\n",
    "        \"prompt\": prompt,\n",
    "        \"gen\": gen or {},\n",
    "    }\n",
    "    with open(request_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(req, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Queued:\", req[\"id\"])\n",
    "    return req[\"id\"]\n",
    "\n",
    "def enqueue_messages(messages, gen=None, req_id=None, request_path=REQUEST_PATH):\n",
    "    req = {\n",
    "        \"id\": req_id or f\"manual_{int(time.time())}_{uuid.uuid4().hex[:6]}\",\n",
    "        \"messages\": messages,\n",
    "        \"gen\": gen or {},\n",
    "    }\n",
    "    with open(request_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(req, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Queued:\", req[\"id\"])\n",
    "    return req[\"id\"]\n",
    "\n",
    "# 示例：随手投递一个请求\n",
    "_ = enqueue_prompt(\n",
    "    \"用三句话解释什么是 overfitting，并举一个简单例子。\",\n",
    "    gen={\"max_new_tokens\": 160, \"temperature\": 0.6, \"top_p\": 0.9},\n",
    ")\n",
    "```\n",
    "\n",
    "### 查询结果\n",
    "```python\n",
    "import json\n",
    "\n",
    "RESULT_PATH = \"grpo_eval_results.jsonl\"\n",
    "\n",
    "def read_last_results(n=3, result_path=RESULT_PATH):\n",
    "    if not os.path.exists(result_path):\n",
    "        print(\"No results yet:\", result_path)\n",
    "        return\n",
    "    lines = open(result_path, \"r\", encoding=\"utf-8\").read().strip().splitlines()\n",
    "    for line in lines[-n:]:\n",
    "        obj = json.loads(line)\n",
    "        print(\"\\n---\")\n",
    "        print(\"ok:\", obj.get(\"ok\"), \"id:\", obj.get(\"id\"), \"step:\", obj.get(\"step\"), \"time:\", obj.get(\"time\"))\n",
    "        if obj.get(\"ok\"):\n",
    "            print(obj.get(\"gen_text_stripped\", obj.get(\"gen_text\", \"\"))[:2000])\n",
    "        else:\n",
    "            print(\"error:\", obj.get(\"error\"))\n",
    "\n",
    "read_last_results(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af2295-bcc2-45f4-a8cf-e3b090be5aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 9) 配置并启动 GRPO\n",
    "# =======================\n",
    "from vllm import SamplingParams\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    temperature = temperature,\n",
    "    top_p = top_p,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    learning_rate = 2e-6,\n",
    "    warmup_ratio = 0.1,\n",
    "    weight_decay = 0.001,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 10,\n",
    "\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "    num_generations = num_generations,\n",
    "    max_prompt_length = 2048,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "\n",
    "    max_steps = max_steps,\n",
    "    save_steps = save_steps,\n",
    "\n",
    "    beta = beta_kl,   # 小 KL 牵引（非 0 才会启用 KL 项）\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"grpo_outputs\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [reward_embedding_similarity],\n",
    "    args = training_args,\n",
    "    train_dataset = raw,\n",
    ")\n",
    "\n",
    "# 加上“按需生成”callback\n",
    "trainer.add_callback(OnDemandGenerateCallback(\n",
    "    tokenizer=tokenizer,\n",
    "    request_path=\"grpo_eval_requests.jsonl\",\n",
    "    result_path=\"grpo_eval_results.jsonl\",\n",
    "    poll_interval_sec=5.0,       # 想更“随时”可设 2.0\n",
    "    max_requests_per_poll=1,     # 单卡建议 1\n",
    "    default_gen_kwargs={         # 默认生成参数（你也可以在请求里覆盖 gen）\n",
    "        \"max_new_tokens\": 192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 20,\n",
    "        \"do_sample\": True,\n",
    "    },\n",
    "    enable_thinking=False, # Qwen3-4B-Thinking-2507的模型卡说过它会始终输出思维链，所以不需要设置True\n",
    "    start_from_end=True,         # 重启训练后从 requests 文件末尾开始读（不回放旧请求）\n",
    "    print_outputs=True,          # 训练日志里直接看到输出\n",
    "    keep_full_text=True, # True: 保存 full_text（含prompt）; False: 仅保存生成部分\n",
    "))\n",
    "\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c9dfb-aeb9-45b4-883b-e7583f722c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 10) 保存 LoRA（你后续可合并导出整模）\n",
    "# =======================\n",
    "model.save_lora(\"grpo_lora_adapter\")\n",
    "print(\"Saved to grpo_lora_adapter/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee27c8-5d6e-46ee-b882-5056cc783568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （可选）合并保存整模（按你的需求选择 merged_16bit / merged_4bit 等）\n",
    "model.save_pretrained_merged(\"grpo_merged_model\", tokenizer, save_method=\"merged_16bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a64a89-34c8-40ad-8616-d9f5cb3f781a",
   "metadata": {},
   "source": [
    "## 11) 导出 GGUF（q4_k_m）\n",
    "说明：\n",
    "- 下面代码默认你此时的 `model` 仍然是 Unsloth 的模型对象，并且 GRPO 的 LoRA 还挂在其上（最推荐的导出时机）。\n",
    "- 如果你已经重启 kernel、只剩下 `grpo_lora_adapter/`，看下一个单元格的“恢复后导出”版本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff854a4e-758d-441d-9131-346daf6f39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 11A) 直接导出 GGUF q4_k_m（推荐：训练结束立刻做）\n",
    "# =======================\n",
    "\n",
    "GGUF_DIR = \"gguf_q4_k_m\"   # 输出目录名\n",
    "# quantization_method 可选： \"q4_k_m\" / \"q8_0\" / \"f16\" 等\n",
    "model.save_pretrained_gguf(\n",
    "    GGUF_DIR,\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\",\n",
    ")\n",
    "\n",
    "print(\"GGUF exported to:\", GGUF_DIR)\n",
    "# 生成的文件通常在该目录下，类似：*Q4_K_M*.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0f013-18f2-47cc-b1f5-07a7bef3c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 防止从hf下载模型，直接用本地模型不联网，否则连不上hf导致失败\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 11B) 如果只剩 LoRA adapter：加载并导出 GGUF q4_k_m\n",
    "# =======================\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "LORA_PATH       = \"grpo_lora_adapter\"     # 你保存的 GRPO LoRA 目录\n",
    "GGUF_DIR        = \"gguf_q4_k_m\"\n",
    "\n",
    "# 加载lora模型\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = LORA_PATH,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = False,  # 导出 GGUF 不需要 vLLM rollout；关掉可少占资源\n",
    "    # local_files_only = True,\n",
    ")\n",
    "\n",
    "# 导出 GGUF q4_k_m\n",
    "model.save_pretrained_gguf(\n",
    "    GGUF_DIR,\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\",\n",
    ")\n",
    "\n",
    "print(\"GGUF exported to:\", GGUF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fdbb1-9391-4cb0-94c3-68adcb6f64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # {\"role\" : \"user\", \"content\" : \"写个故事，讲述了一只小蝌蚪找妈妈的故事。\"}\n",
    "    {\"role\" : \"user\", \"content\" : \"写个故事，讲述了一只小蝌蚪找妈妈的故事。\\n\\n写作规则：先给出一段 Markdown 引用块 `> **综述**`；若内容较长可分段输出，每次只写一段正文并在文末给出 `> **前文提要**`（概括这一段）。只有当全文结束时，最后另起一行写“（完）”。\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Qwen3-4B-Thinking-2507的模型卡说过它会始终输出思维链，所以不需要设置True\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 8192,\n",
    "    # temperature = 1.5, min_p = 0.1,\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
